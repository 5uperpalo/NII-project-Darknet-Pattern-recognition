{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Darknet pattern detection\n",
    "\n",
    "## 1. Pattern detection\n",
    "### 1.1 Define session and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "done with startup\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-oracle-cloudera/jre\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/cloudera/parcels/CDH-6.1.1-1.cdh6.1.1.p0.875250/lib/spark/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"pyspark-shell\"\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark import sql\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext, HiveContext, DataFrameWriter\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, count, sum\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import subprocess\n",
    "import math\n",
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, col, udf\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType, split\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import lit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# described here https://medium.com/@achilleus/spark-session-10d0d66d1d24\n",
    "print(\"starting\")\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"darknet_feature_extraction_without_hive\")\\\n",
    "                    .master(\"yarn\")\\\n",
    "                    .config(\"spark.submit.deployMode\",\"client\")\\\n",
    "                    .config(\"spark.executor.memory\",\"20g\")\\\n",
    "                    .config(\"spark.driver.memory\",\"20g\")\\\n",
    "                    .config('spark.sql.autoBroadcastJoinThreshold','-1')\\\n",
    "                    .enableHiveSupport()\\\n",
    "                    .getOrCreate()\n",
    "\n",
    "print(\"done with startup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the location and structure of files\n",
    "\n",
    "We previosly splitted the traces to 1 hour windows, extracted fields and uploaded them to hdfs.\n",
    "I tried to use window function to split the data in hdfs but it was taking more time than doing it locally on a server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "darknet_pattern_schema = StructType([StructField('frame_time',StringType(),True)\\\n",
    "                                     , StructField('frame_len',StringType(),True)\\\n",
    "                                     , StructField('ip_proto',StringType(),True)\\\n",
    "                                     , StructField('ip_len',StringType(),True)\\\n",
    "                                     , StructField('ip_ttl',StringType(),True)\\\n",
    "                                     , StructField('ip_version',StringType(),True)\\\n",
    "                                     , StructField('ip_flags',StringType(),True)\\\n",
    "                                     , StructField('ip_flags_mf',StringType(),True)\\\n",
    "                                     , StructField('ip_frag_offset',IntegerType(),True)\\\n",
    "                                     , StructField('ip_src',StringType(),True)\\\n",
    "                                     , StructField('ip_dst',StringType(),True)\\\n",
    "                                     , StructField('icmp_type',StringType(),True)\\\n",
    "                                     , StructField('icmp_code',StringType(),True)\\\n",
    "                                     , StructField('tcp_dstport',StringType(),True)\\\n",
    "                                     , StructField('tcp_srcport',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags_ack',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags_cwr',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags_fin',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags_ecn',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags_ns',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags_push',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags_syn',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags_urg',StringType(),True)\\\n",
    "                                     , StructField('tcp_flags_reset',StringType(),True)\\\n",
    "                                     , StructField('tcp_len',StringType(),True)\\\n",
    "                                     , StructField('tcp_window_size',StringType(),True)\\\n",
    "                                     , StructField('udp_srcport',StringType(),True)\\\n",
    "                                     , StructField('udp_dstport',StringType(),True)])\n",
    "\n",
    "hdfs_results_dir = '/user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/'\n",
    "# this is a hdfs directory where we stored 1 hours splits\n",
    "onehour_path = '/user/mulinpav/NII_internship/datasets/darknet/0*2019/extracted_1hour'\n",
    "cmd = 'hdfs dfs -ls ' + onehour_path + \" | sed '1d;s/  */ /g'| cut -d\\  -f8\"\n",
    "\n",
    "files = subprocess.check_output(cmd, shell=True).decode().strip().split('\\n')\n",
    "# this filters out empty strings\n",
    "files = list(filter(None, files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Detect predefined patterns and write results in predefined hdfs directory structure\n",
    "\n",
    "Script in notebook is run only for 1 file and the output is not saved to hdfs. Run the script on background and uncomment lines that include \".write\" to get the reuslts.\n",
    "\n",
    "directory structure:\n",
    "\n",
    "    (base) mulinpav@deep0:~/NII_internship/scripts/big-dama$ hdfs dfs -ls /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour\n",
    "    Found 16 items\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/events\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/net_scan_icmp_heavy_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/net_scan_icmp_light_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/net_scan_tcp_heavy_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/net_scan_tcp_light_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/net_scan_udp_heavy_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/net_scan_udp_light_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/one_flow_tcp_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-06-16 13:47 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/one_flow_udp_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/port_scan_tcp_heavy_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/port_scan_tcp_light_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/port_scan_udp_heavy_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/port_scan_udp_light_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/small_ping_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/small_syn_times\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-07-03 23:13 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/small_udp_times\n",
    "    \n",
    "each directory contains subdirectories with the split name:\n",
    "\n",
    "    (base) mulinpav@deep0:~/NII_internship/scripts/big-dama$ hdfs dfs -ls /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/* | head -4\n",
    "    Found 1022 items\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-06-30 20:21 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/events/20190106000001_csv\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-06-30 20:26 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/events/20190106010001_csv\n",
    "    drwxr-xr-x   - mulinpav mulinpav          0 2019-06-30 20:31 /user/mulinpav/NII_internship/datasets/darknet/time_interval_pattern_analysis/1hour/events/20190106020001_csv\n",
    "\n",
    "Spark works best when it writes output in smaller files/chunks, otherwise it has to gather results to 1 node which slowers it done. Consider results in derecotry '..../events/20190106000001_csv' as '..../events/20190106000001.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#  udfs to help out with identification of some of the attacks\n",
    "def port_scan_tcp_flags_func(x):\n",
    "    # SYN, FIN, FINACK, NULL\n",
    "    flag_list = ['0x00000002','0x00000001','0x00000011','']\n",
    "    if x in flag_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def backscatter_tcp_flags_func(x):\n",
    "    #SYNACK, ACK, RST, RSTACK    \n",
    "    flag_list = ['0x00000012','0x00000010','0x00000004','0x00000014']\n",
    "    if x in flag_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def backscatter_udp_ports_func(x):\n",
    "    #DNS, NTP, NetBIOS, SNMP\n",
    "    port_list = ['53','123','137','161']\n",
    "    if x in port_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "#max_udf = udf(lambda x: np.isin(x, ['0','2','16','18']), StringType())\n",
    "port_scan_tcp_flags_udf = udf(port_scan_tcp_flags_func,IntegerType())\n",
    "backscatter_tcp_flags_udf = udf(backscatter_tcp_flags_func,IntegerType())\n",
    "backscatter_udp_ports_udf = udf(backscatter_udp_ports_func,IntegerType())\n",
    "\n",
    "# threshold values from Darknet paper\n",
    "N1 = 5\n",
    "N2 = 5\n",
    "N3 = 15\n",
    "M = 3\n",
    "R = 0.5\n",
    "\n",
    "for hdfs_file in files[:1]:\n",
    "    file_name = hdfs_file.split('/')[-1]\n",
    "    datadf = spark.read.option(\"sep\", \"\\t\").schema(darknet_pattern_schema).csv(hdfs_file)\n",
    "    # 1. frame_time_ux - timestamps are in format \"Feb 28, 2019 00:00:02.023742000 JST\" - I will convert them to UX timestamp + add ms substring, I ignore the JST as I will be calculating time intervals\n",
    "    # - unix time stamps are used to easily compute time intervals of the attacks\n",
    "    datadf = datadf.withColumn('frame_time_ux', F.unix_timestamp(\"frame_time\", \"MMM dd,yyyy HH:mm:ss\") + F.substring(\"frame_time\", -13, 9).cast('float')/1000000000)\\\n",
    "                   .drop(\"frame_time\")\\\n",
    "                   .fillna(0)\n",
    "    \n",
    "    # portscan\n",
    "    # I added \"(F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range')\"\n",
    "    # and port_scan_tcp_heavy_times\n",
    "    port_scan_tcp_heavy = datadf.filter(F.col('ip_proto')=='6')\\\n",
    "                          .withColumn('port_scan_tcp_flags',port_scan_tcp_flags_udf('tcp_flags'))\\\n",
    "                          .groupby(['ip_src','ip_dst'])\\\n",
    "                          .agg(F.sum('port_scan_tcp_flags'), F.countDistinct('tcp_dstport').alias('distinct_tcp_dstport'), F.count(F.lit(1)).alias('#pkts_ipdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                          .withColumn('port_scan_tcp_flags_frac', F.col('#pkts_ipdst')/F.col('sum(port_scan_tcp_flags)'))\\\n",
    "                          .withColumn('avg_#pkts_tcpdst', F.col('#pkts_ipdst')/F.col('distinct_tcp_dstport'))\\\n",
    "                          .filter((F.col('distinct_tcp_dstport')>=N2) & (F.col('port_scan_tcp_flags_frac')>=R) & (F.col('avg_#pkts_tcpdst')>M))\n",
    "\n",
    "    port_scan_tcp_heavy_times = port_scan_tcp_heavy.select(['event_time_range'])\\\n",
    "                                                   .withColumn('file_name', F.lit(file_name))\n",
    "    #port_scan_tcp_heavy_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'port_scan_tcp_heavy_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    port_scan_tcp_heavy = port_scan_tcp_heavy.withColumn('port_scan_tcp', lit(1))\\\n",
    "                                             .groupby(['ip_src']).agg(F.sum('port_scan_tcp').alias('port_scan_tcp_heavy'))\n",
    "    \n",
    "\n",
    "    port_scan_tcp_light = datadf.filter(F.col('ip_proto')=='6')\\\n",
    "                          .withColumn('port_scan_tcp_flags',port_scan_tcp_flags_udf('tcp_flags'))\\\n",
    "                          .groupby(['ip_src','ip_dst'])\\\n",
    "                          .agg(F.sum('port_scan_tcp_flags'), F.countDistinct('tcp_dstport').alias('distinct_tcp_dstport'), F.count(F.lit(1)).alias('#pkts_ipdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                          .withColumn('port_scan_tcp_flags_frac', F.col('#pkts_ipdst')/F.col('sum(port_scan_tcp_flags)'))\\\n",
    "                          .withColumn('avg_#pkts_tcpdst', F.col('#pkts_ipdst')/F.col('distinct_tcp_dstport'))\\\n",
    "                          .filter((F.col('distinct_tcp_dstport')>=N2) & (F.col('port_scan_tcp_flags_frac')>=R) & (F.col('avg_#pkts_tcpdst')<=M))\n",
    "\n",
    "    port_scan_tcp_light_times = port_scan_tcp_light.select(['event_time_range'])\\\n",
    "                                                   .withColumn('file_name', F.lit(file_name))\n",
    "    #port_scan_tcp_light_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'port_scan_tcp_light_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    port_scan_tcp_light = port_scan_tcp_light.withColumn('port_scan_tcp', lit(1))\\\n",
    "                                             .groupby(['ip_src']).agg(F.sum('port_scan_tcp').alias('port_scan_tcp_light'))\n",
    "\n",
    "\n",
    "    port_scan_udp_heavy = datadf.filter(F.col('ip_proto')=='17')\\\n",
    "                          .groupby(['ip_src','ip_dst'])\\\n",
    "                          .agg(F.countDistinct('udp_dstport').alias('distinct_udp_dstport'), F.count(F.lit(1)).alias('#pkts_ipdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                          .withColumn('avg_#pkts_udpdst', F.col('#pkts_ipdst')/F.col('distinct_udp_dstport'))\\\n",
    "                          .filter((F.col('distinct_udp_dstport')>=N2) & (F.col('avg_#pkts_udpdst')>M))\n",
    "\n",
    "    port_scan_udp_heavy_times = port_scan_udp_heavy.select(['event_time_range'])\\\n",
    "                                                   .withColumn('file_name', F.lit(file_name))\n",
    "    #port_scan_udp_heavy_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'port_scan_udp_heavy_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    port_scan_udp_heavy = port_scan_udp_heavy.withColumn('port_scan_udp', lit(1))\\\n",
    "                                             .groupby(['ip_src']).agg(F.sum('port_scan_udp').alias('port_scan_udp_heavy'))\n",
    "\n",
    "\n",
    "    port_scan_udp_light = datadf.filter(F.col('ip_proto')=='17')\\\n",
    "                          .groupby(['ip_src','ip_dst'])\\\n",
    "                          .agg(F.countDistinct('udp_dstport').alias('distinct_udp_dstport'), F.count(F.lit(1)).alias('#pkts_ipdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                          .withColumn('avg_#pkts_udpdst', F.col('#pkts_ipdst')/F.col('distinct_udp_dstport'))\\\n",
    "                          .filter((F.col('distinct_udp_dstport')>=N2) & (F.col('avg_#pkts_udpdst')<=M))\n",
    "\n",
    "    port_scan_udp_light_times = port_scan_udp_light.select(['event_time_range'])\\\n",
    "                                                   .withColumn('file_name', F.lit(file_name))\n",
    "    #port_scan_udp_light_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'port_scan_udp_light_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    port_scan_udp_light = port_scan_udp_light.withColumn('port_scan_udp', lit(1))\\\n",
    "                                             .groupby(['ip_src']).agg(F.sum('port_scan_udp').alias('port_scan_udp_light'))\n",
    "\n",
    "\n",
    "    # netscan\n",
    "    net_scan_tcp_heavy = datadf.filter(F.col('ip_proto')=='6')\\\n",
    "                         .withColumn('port_scan_tcp_flags',port_scan_tcp_flags_udf('tcp_flags'))\\\n",
    "                         .groupby(['ip_src','tcp_dstport'])\\\n",
    "                         .agg(F.sum('port_scan_tcp_flags'), F.countDistinct('ip_dst').alias('distinct_ip_dst'), F.count(F.lit(1)).alias('#pkts_tcpdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                         .withColumn('port_scan_tcp_flags_frac', F.col('#pkts_tcpdst')/F.col('sum(port_scan_tcp_flags)'))\\\n",
    "                         .withColumn('avg_#pkts_ipdst', F.col('#pkts_tcpdst')/F.col('distinct_ip_dst'))\\\n",
    "                         .filter((F.col('distinct_ip_dst')>=N1) & (F.col('port_scan_tcp_flags_frac')>=R) & (F.col('avg_#pkts_ipdst')>M))\n",
    "\n",
    "    net_scan_tcp_heavy_times = net_scan_tcp_heavy.select(['event_time_range'])\\\n",
    "                                                 .withColumn('file_name', F.lit(file_name))\n",
    "    #net_scan_tcp_heavy_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'net_scan_tcp_heavy_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    net_scan_tcp_heavy = net_scan_tcp_heavy.withColumn('net_scan_tcp', lit(1))\\\n",
    "                                           .groupby(['ip_src']).agg(F.sum('net_scan_tcp').alias('net_scan_tcp_heavy'))\n",
    "\n",
    "    net_scan_tcp_light = datadf.filter(F.col('ip_proto')=='6')\\\n",
    "                         .withColumn('port_scan_tcp_flags',port_scan_tcp_flags_udf('tcp_flags'))\\\n",
    "                         .groupby(['ip_src','tcp_dstport'])\\\n",
    "                         .agg(F.sum('port_scan_tcp_flags'), F.countDistinct('ip_dst').alias('distinct_ip_dst'), F.count(F.lit(1)).alias('#pkts_tcpdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                         .withColumn('port_scan_tcp_flags_frac', F.col('#pkts_tcpdst')/F.col('sum(port_scan_tcp_flags)'))\\\n",
    "                         .withColumn('avg_#pkts_ipdst', F.col('#pkts_tcpdst')/F.col('distinct_ip_dst'))\\\n",
    "                         .filter((F.col('distinct_ip_dst')>=N1) & (F.col('port_scan_tcp_flags_frac')>=R) & (F.col('avg_#pkts_ipdst')<=M))\\\n",
    "\n",
    "    net_scan_tcp_light_times = net_scan_tcp_light.select(['event_time_range'])\\\n",
    "                                                 .withColumn('file_name', F.lit(file_name))\n",
    "    #net_scan_tcp_light_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'net_scan_tcp_light_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    net_scan_tcp_light = net_scan_tcp_light.withColumn('net_scan_tcp', lit(1))\\\n",
    "                                           .groupby(['ip_src']).agg(F.sum('net_scan_tcp').alias('net_scan_tcp_light'))\n",
    "\n",
    "    net_scan_udp_heavy = datadf.filter(F.col('ip_proto')=='17')\\\n",
    "                         .groupby(['ip_src','udp_dstport'])\\\n",
    "                         .agg( F.countDistinct('ip_dst').alias('distinct_ip_dst'), F.count(F.lit(1)).alias('#pkts_udpdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                         .withColumn('avg_#pkts_ipdst', F.col('#pkts_udpdst')/F.col('distinct_ip_dst'))\\\n",
    "                         .filter((F.col('distinct_ip_dst')>=N1) & (F.col('avg_#pkts_ipdst')>M))\n",
    "\n",
    "    net_scan_udp_heavy_times = net_scan_udp_heavy.select(['event_time_range'])\\\n",
    "                                                 .withColumn('file_name', F.lit(file_name))\n",
    "    #net_scan_udp_heavy_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'net_scan_udp_heavy_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    net_scan_udp_heavy = net_scan_udp_heavy.withColumn('net_scan_udp', lit(1))\\\n",
    "                                           .groupby(['ip_src']).agg(F.sum('net_scan_udp').alias('net_scan_udp_heavy'))\n",
    "\n",
    "    net_scan_udp_light = datadf.filter(F.col('ip_proto')=='17')\\\n",
    "                         .groupby(['ip_src','udp_dstport'])\\\n",
    "                         .agg( F.countDistinct('ip_dst').alias('distinct_ip_dst'), F.count(F.lit(1)).alias('#pkts_udpdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                         .withColumn('avg_#pkts_ipdst', F.col('#pkts_udpdst')/F.col('distinct_ip_dst'))\\\n",
    "                         .filter((F.col('distinct_ip_dst')>=N1) & (F.col('avg_#pkts_ipdst')<=M))\n",
    "\n",
    "    net_scan_udp_light_times = net_scan_udp_light.select(['event_time_range'])\\\n",
    "                                                 .withColumn('file_name', F.lit(file_name))\n",
    "    #net_scan_udp_light_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'net_scan_udp_light_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    net_scan_udp_light = net_scan_udp_light.withColumn('net_scan_udp', lit(1))\\\n",
    "                                           .groupby(['ip_src']).agg(F.sum('net_scan_udp').alias('net_scan_udp_light'))\n",
    "\n",
    "\n",
    "    net_scan_icmp_heavy = datadf.filter((F.col('ip_proto')=='1') & (F.col('icmp_type')=='8') & (F.col('icmp_code')=='0'))\\\n",
    "                         .groupby(['ip_src','ip_proto'])\\\n",
    "                         .agg( F.countDistinct('ip_dst').alias('distinct_ip_dst'), F.count(F.lit(1)).alias('#pkts_icmpdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                         .withColumn('avg_#pkts_icmpdst', F.col('#pkts_icmpdst')/F.col('distinct_ip_dst'))\\\n",
    "                         .filter((F.col('distinct_ip_dst')>=N1) & (F.col('avg_#pkts_icmpdst')>M))\n",
    "\n",
    "    net_scan_icmp_heavy_times = net_scan_icmp_heavy.select(['event_time_range'])\\\n",
    "                                                   .withColumn('file_name', F.lit(file_name))\n",
    "    #net_scan_icmp_heavy_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'net_scan_icmp_heavy_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    net_scan_icmp_heavy = net_scan_icmp_heavy.withColumn('net_scan_icmp_heavy', lit(1))\\\n",
    "                                           .select(['ip_src','net_scan_icmp_heavy'])\n",
    "\n",
    "    net_scan_icmp_light = datadf.filter((F.col('ip_proto')=='1') & (F.col('icmp_type')=='8') & (F.col('icmp_code')=='0'))\\\n",
    "                         .groupby(['ip_src','ip_proto'])\\\n",
    "                         .agg( F.countDistinct('ip_dst').alias('distinct_ip_dst'), F.count(F.lit(1)).alias('#pkts_icmpdst'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                         .withColumn('avg_#pkts_icmpdst', F.col('#pkts_icmpdst')/F.col('distinct_ip_dst'))\\\n",
    "                         .filter((F.col('distinct_ip_dst')>=N1) & (F.col('avg_#pkts_icmpdst')<=M))\n",
    "\n",
    "    net_scan_icmp_light_times = net_scan_icmp_light.select(['event_time_range'])\\\n",
    "                                                   .withColumn('file_name', F.lit(file_name))    \n",
    "    #net_scan_icmp_light_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'net_scan_icmp_light_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "\n",
    "    net_scan_icmp_light = net_scan_icmp_light.withColumn('net_scan_icmp_light', lit(1))\\\n",
    "                                           .select(['ip_src','net_scan_icmp_light'])\n",
    "\n",
    "    # oneflow\n",
    "    one_flow_tcp = datadf.filter(F.col('ip_proto')=='6')\\\n",
    "                         .groupby(['ip_src','ip_dst','tcp_dstport'])\\\n",
    "                         .agg(F.count(F.lit(1)).alias('#pkts'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                         .filter(F.col('#pkts')>N3)\\\n",
    "                         .withColumn('one_flow_tcp', lit(1))\n",
    "    one_flow_tcp_times = one_flow_tcp.select(['event_time_range'])\\\n",
    "                                     .withColumn('file_name', F.lit(file_name))\n",
    "\n",
    "    #one_flow_tcp_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'one_flow_tcp_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "    one_flow_tcp = one_flow_tcp.select(['ip_src','one_flow_tcp'])\\\n",
    "                               .groupby(['ip_src']).agg(F.sum('one_flow_tcp').alias('one_flow_tcp'))\n",
    "\n",
    "    one_flow_udp = datadf.filter(F.col('ip_proto')=='17')\\\n",
    "                         .groupby(['ip_src','ip_dst','udp_dstport'])\\\n",
    "                         .agg(F.count(F.lit(1)).alias('#pkts'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                         .filter(F.col('#pkts')>N3)\\\n",
    "                         .withColumn('one_flow_udp', lit(1))\n",
    "    one_flow_udp_times = one_flow_udp.select(['event_time_range'])\\\n",
    "                                     .withColumn('file_name', F.lit(file_name))\n",
    "    #one_flow_udp_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'one_flow_udp_times'+ file_name +'_csv', mode='append', sep=',')\n",
    "    one_flow_udp = one_flow_udp.select(['ip_src','one_flow_udp'])\\\n",
    "                               .groupby(['ip_src']).agg(F.sum('one_flow_udp').alias('one_flow_udp'))\n",
    "\n",
    "    # backscatter\n",
    "    # I did not include time range checking here as its at least 1 packet\n",
    "    # check if tcp_flags in hexadecimal corresponds to separately extracted flags\n",
    "    backscatter_tcp = datadf.filter(F.col('ip_proto')=='6')\\\n",
    "                            .withColumn('backscatter_tcp_flags',backscatter_tcp_flags_udf('tcp_flags'))\\\n",
    "                            .filter(F.col('backscatter_tcp_flags')==1)\\\n",
    "                            .groupby(['ip_src'])\\\n",
    "                            .agg(F.count(F.lit(1)).alias('#pkts'))\\\n",
    "                            .filter(F.col('#pkts')>1)\\\n",
    "                            .withColumn('backscatter_tcp', lit(1))\\\n",
    "                            .select(['ip_src','backscatter_tcp'])\n",
    "\n",
    "    backscatter_udp = datadf.filter(F.col('ip_proto')=='17')\\\n",
    "                            .withColumn('backscatter_udp_ports',backscatter_udp_ports_udf('udp_srcport'))\\\n",
    "                            .filter(F.col('backscatter_udp_ports')==1)\\\n",
    "                            .groupby(['ip_src'])\\\n",
    "                            .agg(F.count(F.lit(1)).alias('#pkts'))\\\n",
    "                            .filter(F.col('#pkts')>1)\\\n",
    "                            .withColumn('backscatter_udp', lit(1))\\\n",
    "                            .select(['ip_src','backscatter_udp'])                        \n",
    "\n",
    "    backscatter_icmp = datadf.filter((F.col('ip_proto')=='1') & (((F.col('icmp_type')=='0') & (F.col('icmp_code')=='0')) | (F.col('icmp_type')=='3') | ((F.col('icmp_type')=='11') & (F.col('icmp_code')=='0'))))\\\n",
    "                            .groupby(['ip_src'])\\\n",
    "                            .agg(F.count(F.lit(1)).alias('#pkts'))\\\n",
    "                            .filter(F.col('#pkts')>1)\\\n",
    "                            .withColumn('backscatter_icmp', lit(1))\\\n",
    "                            .select(['ip_src','backscatter_icmp'])                                                \n",
    "\n",
    "    # fragmentation ip.flags == 0x01 or ip.frag_offset > 0\n",
    "    # https:/osqa-ask.wireshark.org/questions/41152/how-to-check-if-fragmentation-is-happening\n",
    "    # cast(ip_flags as string) as ip_flags,\\\n",
    "    # cast(ip_frag_offset as integer) as ip_frag_offset,\\\n",
    "    # I did not include time range checking here as its at least 1 packet\n",
    "    ip_fragment = datadf.filter((F.col('ip_flags_mf')=='1') | (F.col('ip_frag_offset')>0))\\\n",
    "                        .groupby(['ip_src'])\\\n",
    "                        .agg(F.count(F.lit(1)).alias('#pkts'))\\\n",
    "                        .filter(F.col('#pkts')>0)\\\n",
    "                        .withColumn('ip_fragment', lit(1))\\\n",
    "                        .select(['ip_src','ip_fragment'])\n",
    "\n",
    "    # smallSYN, smallUDP, smallPING\n",
    "    small_syn = datadf.filter(F.col('tcp_flags')=='0x00000002')\\\n",
    "                      .groupby(['ip_src'])\\\n",
    "                      .agg(F.countDistinct('ip_dst').alias('distinct_ip_dst'), F.countDistinct('tcp_dstport').alias('distinct_tcp_dstports'), F.count(F.lit(1)).alias('#pkts'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                      .filter((F.col('distinct_ip_dst')<N1) & (F.col('distinct_tcp_dstports')<N2) & (F.col('#pkts')<=N3))\\\n",
    "                      .withColumn('small_syn', lit(1))\n",
    "\n",
    "    small_syn_times = small_syn.select(['event_time_range'])\\\n",
    "                               .withColumn('file_name', F.lit(file_name))\n",
    "    #small_syn_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'small_syn_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "    small_syn = small_syn.select(['ip_src','small_syn'])\n",
    "\n",
    "    small_udp = datadf.filter(F.col('ip_proto')=='17')\\\n",
    "                      .groupby(['ip_src'])\\\n",
    "                      .agg(F.countDistinct('ip_dst').alias('distinct_ip_dst'), F.countDistinct('udp_dstport').alias('distinct_udp_dstports'), F.count(F.lit(1)).alias('#pkts'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                      .filter((F.col('distinct_ip_dst')<N1) & (F.col('distinct_udp_dstports')<N2) & (F.col('#pkts')<=N3))\\\n",
    "                      .withColumn('small_udp', lit(1))\n",
    "    small_udp_times = small_udp.select(['event_time_range'])\\\n",
    "                               .withColumn('file_name', F.lit(file_name))\n",
    "    #small_udp_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'small_udp_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "    small_udp = small_udp.select(['ip_src','small_udp'])\n",
    "\n",
    "    small_ping = datadf.filter((F.col('ip_proto')=='1') & (F.col('icmp_type')=='8') & (F.col('icmp_code')=='0'))\\\n",
    "                       .groupby(['ip_src'])\\\n",
    "                       .agg(F.countDistinct('ip_dst').alias('distinct_ip_dst'), F.count(F.lit(1)).alias('#pkts'), (F.max('frame_time_ux')-F.min('frame_time_ux')).alias('event_time_range'))\\\n",
    "                       .filter((F.col('distinct_ip_dst')<N1) & (F.col('#pkts')<=N3))\\\n",
    "                       .withColumn('small_ping', lit(1))\n",
    "    small_ping_times = small_ping.select(['event_time_range'])\\\n",
    "                                 .withColumn('file_name', F.lit(file_name))\n",
    "    #small_ping_times.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'small_ping_times/'+ file_name +'_csv', mode='append', sep=',')\n",
    "    small_ping = small_ping.select(['ip_src','small_ping']) \n",
    "\n",
    "    # according to paper \"An Evaluation of Darknet traffic Taxonomy\" following traffic may overlap with \"other traffic\":\n",
    "    # ['backscatter_tcp', 'backscatter_udp', 'backscatter_icmp', 'ip_fragment']\n",
    "    # so we summ all ip_src,traffic pattern tuples except these 4 and mark them with ['other_tcp', 'other_udp', 'other_icmp','other']\n",
    "    non_other_list = ['port_scan_tcp_heavy', 'port_scan_tcp_light', 'port_scan_udp_heavy', 'port_scan_udp_light', 'net_scan_tcp_heavy', 'net_scan_tcp_light', 'net_scan_udp_heavy', 'net_scan_udp_light', 'net_scan_icmp_heavy', 'net_scan_icmp_light', 'one_flow_tcp', 'one_flow_udp', 'small_syn', 'small_udp', 'small_ping']\n",
    "\n",
    "    result = datadf.select(['ip_src']).distinct()\\\n",
    "                        .join(port_scan_tcp_heavy, 'ip_src',how='left')\\\n",
    "                        .join(port_scan_tcp_light, 'ip_src',how='left')\\\n",
    "                        .join(port_scan_udp_heavy, 'ip_src',how='left')\\\n",
    "                        .join(port_scan_udp_light, 'ip_src',how='left')\\\n",
    "                        .join(net_scan_tcp_heavy, 'ip_src',how='left')\\\n",
    "                        .join(net_scan_tcp_light, 'ip_src',how='left')\\\n",
    "                        .join(net_scan_udp_heavy, 'ip_src',how='left')\\\n",
    "                        .join(net_scan_udp_light, 'ip_src',how='left')\\\n",
    "                        .join(net_scan_icmp_heavy, 'ip_src',how='left')\\\n",
    "                        .join(net_scan_icmp_light, 'ip_src',how='left')\\\n",
    "                        .join(one_flow_tcp, 'ip_src',how='left')\\\n",
    "                        .join(one_flow_udp, 'ip_src',how='left')\\\n",
    "                        .join(backscatter_tcp, 'ip_src',how='left')\\\n",
    "                        .join(backscatter_udp, 'ip_src',how='left')\\\n",
    "                        .join(backscatter_icmp, 'ip_src',how='left')\\\n",
    "                        .join(ip_fragment, 'ip_src',how='left')\\\n",
    "                        .join(small_syn, 'ip_src',how='left')\\\n",
    "                        .join(small_udp, 'ip_src',how='left')\\\n",
    "                        .join(small_ping, 'ip_src',how='left')\\\n",
    "                        .fillna(0)\\\n",
    "                        .withColumn('other_temp' ,reduce(add, [F.col(x) for x in non_other_list]))\n",
    "    #other = result_temp.filter\n",
    "    # takes the remaining 'other ip_src' joins them with ip_src of tcp,udp,icmp, sums their value and if the sum is still 0 then it will put 1 into 'other' column\n",
    "    other_tcp = datadf.filter(F.col('ip_proto')=='6').select('ip_src').distinct().withColumn('other_tcp', lit(1))\n",
    "    other_udp = datadf.filter(F.col('ip_proto')=='17').select('ip_src').distinct().withColumn('other_udp', lit(1))\n",
    "    other_icmp = datadf.filter(F.col('ip_proto')=='1').select('ip_src').distinct().withColumn('other_icmp', lit(1))\n",
    "    other_temp = result.filter(F.col('other_temp')==0).select(['ip_src'])\\\n",
    "                       .join(other_tcp,'ip_src',how='left')\\\n",
    "                       .join(other_udp,'ip_src',how='left')\\\n",
    "                       .join(other_icmp,'ip_src',how='left')\\\n",
    "                       .fillna(0)\\\n",
    "                       .withColumn('other' ,reduce(add, [F.col(x) for x in ['other_tcp', 'other_udp', 'other_icmp']]))\\\n",
    "                       .withColumn('other', F.when(F.col('other') > 0, 0).otherwise(1))\n",
    "\n",
    "    result = result.join(other_temp,'ip_src',how='left').drop('other_temp').fillna(0)\\\n",
    "                   .withColumn('file_name', F.lit(file_name))\n",
    "    #result.write.format(\"com.databricks.spark.csv\").option('header', 'true').save(path=hdfs_results_dir+ 'events/'+ file_name +'_csv', mode='append', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Example outputs\n",
    "### 1.4.1 Matrix of detected anolies/patterns in 1 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+------------+------------+---------------+---------------+----------------+-----------+---------+---------+----------+---------+---------+----------+-----+--------------+\n",
      "|              ip_src|port_scan_tcp_heavy|port_scan_tcp_light|port_scan_udp_heavy|port_scan_udp_light|net_scan_tcp_heavy|net_scan_tcp_light|net_scan_udp_heavy|net_scan_udp_light|net_scan_icmp_heavy|net_scan_icmp_light|one_flow_tcp|one_flow_udp|backscatter_tcp|backscatter_udp|backscatter_icmp|ip_fragment|small_syn|small_udp|small_ping|other_tcp|other_udp|other_icmp|other|     file_name|\n",
      "+--------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+------------+------------+---------------+---------------+----------------+-----------+---------+---------+----------+---------+---------+----------+-----+--------------+\n",
      "|       1.172.211.122|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|         1.186.207.9|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|         1.64.220.29|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|       101.227.67.99|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|     101.231.238.148|                  0|                  0|                  0|                  0|                 0|                 1|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        0|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|        101.51.9.242|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|       101.71.151.44|                  0|                  0|                  0|                  0|                 0|                 1|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        0|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|       103.10.134.54|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|      103.101.107.11|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|      103.105.107.23|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|       103.11.68.118|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|     103.113.105.178|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|      103.119.25.246|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|        103.15.81.87|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|     103.206.103.104|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|103.208.220.131,1...|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        0|        0|         0|        0|        0|         0|    1|20190106000001|\n",
      "|     103.209.178.157|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|      103.254.56.235|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|      103.38.201.140|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "|       103.43.122.21|                  0|                  0|                  0|                  0|                 0|                 0|                 0|                 0|                  0|                  0|           0|           0|              0|              0|               0|          0|        1|        0|         0|        0|        0|         0|    0|20190106000001|\n",
      "+--------------------+-------------------+-------------------+-------------------+-------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+------------+------------+---------------+---------------+----------------+-----------+---------+---------+----------+---------+---------+----------+-----+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Time intervals of all pattern types - for further time analysis of the patterns (backscatter tcp/udp/icmp and IP fragments are not analysed as they are \"1 packet patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "|1330.7212989330292|20190106000001|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port_scan_tcp_heavy_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "| 2281.438353061676|20190106000001|\n",
      "|3254.2540678977966|20190106000001|\n",
      "| 3431.640604019165|20190106000001|\n",
      "|2712.7043359279633|20190106000001|\n",
      "| 3168.871365070343|20190106000001|\n",
      "|1906.3141779899597|20190106000001|\n",
      "|2741.9848840236664|20190106000001|\n",
      "|1785.6969730854034|20190106000001|\n",
      "|2625.4189579486847|20190106000001|\n",
      "| 2639.019593000412|20190106000001|\n",
      "| 1950.283979177475|20190106000001|\n",
      "|1536.5958559513092|20190106000001|\n",
      "|2808.8331730365753|20190106000001|\n",
      "| 3216.664701938629|20190106000001|\n",
      "|1639.5598258972168|20190106000001|\n",
      "|3391.4076359272003|20190106000001|\n",
      "|3406.5028550624847|20190106000001|\n",
      "|2859.0861370563507|20190106000001|\n",
      "|1537.1859271526337|20190106000001|\n",
      "|2920.9726588726044|20190106000001|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port_scan_tcp_light_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+\n",
      "|event_time_range|file_name|\n",
      "+----------------+---------+\n",
      "+----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port_scan_udp_heavy_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "| 2889.072543859482|20190106000001|\n",
      "| 3136.098750114441|20190106000001|\n",
      "|2188.1759221553802|20190106000001|\n",
      "|1874.8825759887695|20190106000001|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "port_scan_udp_light_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "|  71.2768280506134|20190106000001|\n",
      "| 3589.927623987198|20190106000001|\n",
      "| 3592.637727022171|20190106000001|\n",
      "|2538.5068020820618|20190106000001|\n",
      "|3512.1067910194397|20190106000001|\n",
      "| 3589.951479911804|20190106000001|\n",
      "| 3068.463495016098|20190106000001|\n",
      "|3585.8166489601135|20190106000001|\n",
      "|3589.6720900535583|20190106000001|\n",
      "| 2983.540219068527|20190106000001|\n",
      "| 3583.706946849823|20190106000001|\n",
      "| 3361.326663017273|20190106000001|\n",
      "|2728.0358130931854|20190106000001|\n",
      "|3094.8821699619293|20190106000001|\n",
      "|1987.9059798717499|20190106000001|\n",
      "|187.95389890670776|20190106000001|\n",
      "| 37.19043493270874|20190106000001|\n",
      "|3547.5861341953278|20190106000001|\n",
      "|3221.2486419677734|20190106000001|\n",
      "|2624.8135719299316|20190106000001|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_scan_tcp_heavy_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "| 3500.263459920883|20190106000001|\n",
      "| 3596.805120944977|20190106000001|\n",
      "|2112.5043399333954|20190106000001|\n",
      "| 3596.809244155884|20190106000001|\n",
      "| 469.7812268733978|20190106000001|\n",
      "|2885.4059369564056|20190106000001|\n",
      "| 3536.493569135666|20190106000001|\n",
      "|3519.8709490299225|20190106000001|\n",
      "|3578.2405819892883|20190106000001|\n",
      "|229.62878108024597|20190106000001|\n",
      "|3020.6466159820557|20190106000001|\n",
      "| 410.7451660633087|20190106000001|\n",
      "|  3384.04843378067|20190106000001|\n",
      "|   721.03187084198|20190106000001|\n",
      "| 3498.526953935623|20190106000001|\n",
      "| 3325.023730993271|20190106000001|\n",
      "| 3474.014904022217|20190106000001|\n",
      "|2988.2877311706543|20190106000001|\n",
      "|3552.5137569904327|20190106000001|\n",
      "| 3262.887512922287|20190106000001|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_scan_tcp_light_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "| 66.67015600204468|20190106000001|\n",
      "|3526.7240540981293|20190106000001|\n",
      "|3044.3941020965576|20190106000001|\n",
      "|3173.6300139427185|20190106000001|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_scan_udp_heavy_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "| 2993.124549150467|20190106000001|\n",
      "| 3392.343747854233|20190106000001|\n",
      "|2606.8581869602203|20190106000001|\n",
      "|2274.2927129268646|20190106000001|\n",
      "| 2554.562464952469|20190106000001|\n",
      "| 3294.096009016037|20190106000001|\n",
      "|3172.6657869815826|20190106000001|\n",
      "|  2244.51220202446|20190106000001|\n",
      "|2908.9770181179047|20190106000001|\n",
      "| 2688.988273859024|20190106000001|\n",
      "| 1251.173772096634|20190106000001|\n",
      "| 2704.412784099579|20190106000001|\n",
      "|1792.4399709701538|20190106000001|\n",
      "| 2874.548471927643|20190106000001|\n",
      "|  757.408280134201|20190106000001|\n",
      "| 2217.127156972885|20190106000001|\n",
      "|3556.0651988983154|20190106000001|\n",
      "|418.13468074798584|20190106000001|\n",
      "| 2476.482820034027|20190106000001|\n",
      "|1965.3743669986725|20190106000001|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_scan_udp_light_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "| 3167.946217060089|20190106000001|\n",
      "|179.09645199775696|20190106000001|\n",
      "+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_scan_icmp_heavy_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "|2815.7282400131226|20190106000001|\n",
      "|  2110.17214679718|20190106000001|\n",
      "| 3276.595118999481|20190106000001|\n",
      "| 960.4653151035309|20190106000001|\n",
      "| 2779.487086057663|20190106000001|\n",
      "| 3308.157867193222|20190106000001|\n",
      "|3011.1383361816406|20190106000001|\n",
      "|  3429.30957698822|20190106000001|\n",
      "|2409.0330572128296|20190106000001|\n",
      "| 2463.563570022583|20190106000001|\n",
      "| 2292.224575996399|20190106000001|\n",
      "| 3106.133035182953|20190106000001|\n",
      "|3160.9027009010315|20190106000001|\n",
      "|1712.1145000457764|20190106000001|\n",
      "| 2653.809695005417|20190106000001|\n",
      "| 2896.881795167923|20190106000001|\n",
      "|3091.2760059833527|20190106000001|\n",
      "|11.611090183258057|20190106000001|\n",
      "| 814.7871007919312|20190106000001|\n",
      "|2375.9059801101685|20190106000001|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net_scan_icmp_light_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "| 3516.288110971451|20190106000001|\n",
      "| 3511.203238964081|20190106000001|\n",
      "| 3457.195995092392|20190106000001|\n",
      "| 3039.945447921753|20190106000001|\n",
      "| 3599.205692052841|20190106000001|\n",
      "|3457.2062969207764|20190106000001|\n",
      "|3214.2094078063965|20190106000001|\n",
      "| 3457.195519924164|20190106000001|\n",
      "|3376.2172000408173|20190106000001|\n",
      "| 3457.225975036621|20190106000001|\n",
      "| 3403.204456090927|20190106000001|\n",
      "|3457.1991159915924|20190106000001|\n",
      "|3528.0031819343567|20190106000001|\n",
      "|3511.2062838077545|20190106000001|\n",
      "|3497.1824049949646|20190106000001|\n",
      "|3573.3697719573975|20190106000001|\n",
      "|  3061.19438290596|20190106000001|\n",
      "| 3457.198278903961|20190106000001|\n",
      "| 3189.848515033722|20190106000001|\n",
      "|3457.2681579589844|20190106000001|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_flow_tcp_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "|1065.6567652225494|20190106000001|\n",
      "| 1395.960561990738|20190106000001|\n",
      "|3505.6968047618866|20190106000001|\n",
      "|481.37702679634094|20190106000001|\n",
      "| 3511.761064052582|20190106000001|\n",
      "|112.91864204406738|20190106000001|\n",
      "| 188.5094771385193|20190106000001|\n",
      "|   625.98566198349|20190106000001|\n",
      "| 985.4595642089844|20190106000001|\n",
      "|2511.5937390327454|20190106000001|\n",
      "|1347.6298308372498|20190106000001|\n",
      "|1157.3182830810547|20190106000001|\n",
      "|2474.8983612060547|20190106000001|\n",
      "| 2866.831038951874|20190106000001|\n",
      "|3388.0810618400574|20190106000001|\n",
      "|477.09853196144104|20190106000001|\n",
      "|1840.6707060337067|20190106000001|\n",
      "|411.54975986480713|20190106000001|\n",
      "|3224.9102380275726|20190106000001|\n",
      "|1446.8420329093933|20190106000001|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_flow_udp_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "|               0.0|20190106000001|\n",
      "| 2808.662714958191|20190106000001|\n",
      "|1417.2683730125427|20190106000001|\n",
      "|1049.9989929199219|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "| 992.8346269130707|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|386.59739804267883|20190106000001|\n",
      "| 990.1242680549622|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "| 959.0696110725403|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_syn_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|    event_time_range|     file_name|\n",
      "+--------------------+--------------+\n",
      "|  1323.5709879398346|20190106000001|\n",
      "|  0.5906028747558594|20190106000001|\n",
      "|0.002569913864135742|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "|   47.38193893432617|20190106000001|\n",
      "|   9.932496070861816|20190106000001|\n",
      "|  18.002815008163452|20190106000001|\n",
      "|0.001015186309814...|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "| 0.34627509117126465|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "|   74.38310813903809|20190106000001|\n",
      "|  3.4490578174591064|20190106000001|\n",
      "|                 0.0|20190106000001|\n",
      "+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_udp_times.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|  event_time_range|     file_name|\n",
      "+------------------+--------------+\n",
      "|1057.4914350509644|20190106000001|\n",
      "| 600.0400061607361|20190106000001|\n",
      "| 1224.664668083191|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "| 182.6839461326599|20190106000001|\n",
      "| 10.50426697731018|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "| 1618.070030927658|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "| 1850.462163925171|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "| 2464.486990213394|20190106000001|\n",
      "| 4.996448040008545|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "| 1107.020674943924|20190106000001|\n",
      "|               0.0|20190106000001|\n",
      "|1991.4086718559265|20190106000001|\n",
      "+------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "small_ping_times.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
